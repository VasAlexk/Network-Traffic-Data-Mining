import os
import pandas as pd
import numpy as np
from hdbscan import HDBSCAN
from sklearn.utils import shuffle

# === Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ ===
input_file = "pca_transformed_data_with_labels.csv"
output_dir = "hdbscan_chunks_output"
os.makedirs(output_dir, exist_ok=True)

chunk_size = 1_000_000
num_chunks = 9
min_cluster_size = 50
points_per_cluster = 5
rare_threshold = 1500
random_state = 42

np.random.seed(random_state)

# === Î”Î¹Î¬Î²Î±ÏƒÎ¼Î± ÏƒÏ„Î·Î»ÏÎ½ ===
with open(input_file, "r", encoding="utf-8") as f:
    header = f.readline().strip().split(",")

pca_cols = [col for col in header if col.startswith("PC")]

# === Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± rare types ÎºÎ±Î¹ fallback ===
print("Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")
df_all = pd.read_csv(input_file)

# === Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ€Î¬Î½Î¹Ï‰Î½ Traffic Types ===
rare_traffic_types = df_all["Traffic Type"].value_counts()
rare_traffic_types = rare_traffic_types[rare_traffic_types < rare_threshold].index.tolist()
rare_rows = df_all[df_all["Traffic Type"].isin(rare_traffic_types)].copy()

print(f"\nÎ£Ï€Î¬Î½Î¹Î± Traffic Types (< {rare_threshold}): {len(rare_rows)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î±Ï€ÏŒ {len(rare_traffic_types)} Ï„ÏÏ€Î¿Ï…Ï‚")

# === Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ·: Î•Ï€Î¹Î»Î¿Î³Î® top-N ÏƒÎ·Î¼ÎµÎ¯Ï‰Î½ Î±Î½Î¬ cluster ===
def find_top_n_points_per_cluster(df, pca_cols, n=5):
    selected = []
    for _, group in df.groupby("Cluster"):
        X = group[pca_cols].to_numpy()
        centroid = np.mean(X, axis=0)
        group = group.copy()
        group["dist"] = np.linalg.norm(X - centroid, axis=1)
        top_n = group.nsmallest(n, "dist")
        selected.append(top_n.drop(columns=["dist"]))
    return pd.concat(selected)

# === Î’Î®Î¼Î± 1: HDBSCAN ÏƒÎµ chunks ===
chunk_paths = []

for i in range(num_chunks):
    skip = i * chunk_size + 1
    print(f"\nğŸ”¹ [Chunk {i+1}] Î“ÏÎ±Î¼Î¼Î­Ï‚ {skip:,} â€“ {skip + chunk_size - 1:,}")

    try:
        chunk = pd.read_csv(input_file, skiprows=skip, nrows=chunk_size, header=None, names=header)
        X_chunk = chunk[pca_cols]

        hdb = HDBSCAN(min_cluster_size=min_cluster_size)
        chunk["Cluster"] = hdb.fit_predict(X_chunk)

        clustered = chunk[chunk["Cluster"] != -1].copy()
        clustered["Cluster"] += (i + 1) * 1000

        selected_df = find_top_n_points_per_cluster(clustered, pca_cols, n=points_per_cluster)

        chunk_path = os.path.join(output_dir, f"chunk_{i+1}_selected.csv")
        selected_df.to_csv(chunk_path, index=False)
        chunk_paths.append(chunk_path)

        print(f"Î•Ï€Î¹Î»Î­Ï‡Î¸Î·ÎºÎ±Î½ {selected_df.shape[0]:,} ÏƒÎ·Î¼ÎµÎ¯Î± â†’ {chunk_path}")

    except Exception as e:
        print(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ chunk {i+1}: {e}")

# === Î’Î®Î¼Î± 2: Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ selected chunks ===
print("\nÎ£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· chunks...")
dfs = [pd.read_csv(f) for f in chunk_paths]
merged_df = pd.concat(dfs, ignore_index=True)
print(f"Î•Ï€Î¹Î»ÎµÎ³Î¼Î­Î½Î± ÏƒÎ·Î¼ÎµÎ¯Î± Î±Ï€ÏŒ clustering: {len(merged_df):,}")

# === Î’Î®Î¼Î± 3: Oversampling Label Minority (ÏŒÏ‡Î¹ SMOTE)
label_counts = merged_df["Label"].value_counts()
print(f"\nÎšÎ±Ï„Î±Î½Î¿Î¼Î­Ï‚ Label Ï€ÏÎ¹Î½ oversampling: {label_counts.to_dict()}")

if "Benign" in label_counts and "Malicious" in label_counts:
    majority = label_counts.idxmax()
    minority = label_counts.idxmin()

    df_major = merged_df[merged_df["Label"] == majority]
    df_minor = merged_df[merged_df["Label"] == minority]

    if len(df_minor) < 2000:
        df_minor_oversampled = df_minor.sample(n=2000, replace=True, random_state=random_state)
        merged_df = pd.concat([df_major, df_minor_oversampled], ignore_index=True)
        print(f"Oversampling {minority} â†’ 2000 Î´ÎµÎ¯Î³Î¼Î±Ï„Î±")
    else:
        print("Labels Î®Î´Î· Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î±.")
else:
    print("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÎºÎ±Î¹ Î¿Î¹ Î´ÏÎ¿ Label â€“ Ï€Î±ÏÎ¬ÎºÎ±Î¼ÏˆÎ· oversampling.")

# === Î’Î®Î¼Î± 4: Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· rare traffic types ===
print(f"\nÎ ÏÎ¿ÏƒÎ¸Î®ÎºÎ· {len(rare_rows):,} Î³ÏÎ±Î¼Î¼ÏÎ½ Î±Ï€ÏŒ ÏƒÏ€Î¬Î½Î¹Î± Traffic Types")
merged_df = pd.concat([merged_df, rare_rows], ignore_index=True)

# === Î’Î®Î¼Î± 4b: Duplication Î³Î¹Î± Traffic Types Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ <500 ÏƒÏ„Î¿ merged_df ===
print("\nDuplication Î³Î¹Î± Traffic Types Î¼Îµ <500 Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÏƒÏ„Î¿ merged_df...")

# Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Ï…Î¼Îµ Ï€Î»Î®Î¸Î¿Ï‚ ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÏ‰Î½ Î±Î½Î¬ traffic type ÏƒÏ„Î¿ merged_df
merged_counts = merged_df["Traffic Type"].value_counts()
under_500 = merged_counts[merged_counts < 500]

for traffic_type, current_len in under_500.items():
    # Î ÏŒÏƒÎ± Î­Ï‡ÎµÎ¹ ÏƒÏ„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ df_all
    total_in_all = df_all[df_all["Traffic Type"] == traffic_type].shape[0]

    # Î‘Î½ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ Î­Ï‡ÎµÎ¹ <= 500 â†’ ÏƒÏ„ÏŒÏ‡Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ 500
    if total_in_all <= 500:
        target_n = 500
    else:
        # Î‘Î½ Î­Ï‡ÎµÎ¹ >500 â†’ Î±ÏÎ¾Î·ÏƒÎ­ Ï„Î¿ Î¼ÎµÏ‡ÏÎ¹ 1000
         target_n = min(1000, total_in_all)

    needed = target_n - current_len
    if needed <= 0:
        continue

    df_subset = merged_df[merged_df["Traffic Type"] == traffic_type]
    
    if df_subset.empty:
        print(f"Î¤Î¿ '{traffic_type}' Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ„Î¿ merged_df â€“ Î±Î³Î½Î¿ÎµÎ¯Ï„Î±Î¹.")
        continue

    duplicated = df_subset.sample(n=needed, replace=True, random_state=random_state)
    merged_df = pd.concat([merged_df, duplicated], ignore_index=True)
    print(f"{traffic_type}': {current_len} â†’ {current_len + needed} (ÏƒÏ„ÏŒÏ‡Î¿Ï‚: {target_n})")


# === Î’Î®Î¼Î± 4c: ÎœÎµÎ¯Ï‰ÏƒÎ· Malicious + DoS ÎºÎ±Ï„Î¬ 20% ===
print("\nÎœÎµÎ¯Ï‰ÏƒÎ· Ï„Ï‰Î½ Î³ÏÎ±Î¼Î¼ÏÎ½ Î¼Îµ Label='Malicious' ÎºÎ±Î¹ Traffic Type='DoS' ÎºÎ±Ï„Î¬ 20%...")
mask_malicious_dos = (merged_df["Label"] == "Malicious") & (merged_df["Traffic Type"] == "DoS")
df_malicious_dos = merged_df[mask_malicious_dos]
keep_n = int(len(df_malicious_dos) * 0.8)

df_malicious_dos_reduced = df_malicious_dos.sample(n=keep_n, random_state=random_state)
merged_df = pd.concat([merged_df[~mask_malicious_dos], df_malicious_dos_reduced], ignore_index=True)

print(f"Malicious-DoS Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼ÎµÎ¹ÏÎ¸Î·ÎºÎ±Î½ ÏƒÎµ {keep_n}")

# === Î’Î®Î¼Î± 5: Shuffle & Save ===
merged_df = shuffle(merged_df, random_state=random_state)
merged_df = merged_df.drop(columns=["Cluster"], errors="ignore")
final_path = os.path.join(output_dir, "final_hdbscan_balanced.csv")
merged_df.to_csv(final_path, index=False)

print(f"\nÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ! Î¤ÎµÎ»Î¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿: {final_path} Î¼Îµ {len(merged_df):,} Î³ÏÎ±Î¼Î¼Î­Ï‚")
